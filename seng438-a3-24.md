**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report #3 – Code Coverage, Adequacy Criteria and Test Case Correlation**

| Group \#:      |  24   |
| -------------- | --- |
| Student Names: |  Ella Boulanger   |
|                |  Kenzie Fjestad   |
|                |  Raina Jugdev   |
|                |  Dominico Mendes   |

(Note that some labs require individual reports while others require one report
for each group. Please see each lab document for details.)

# 1 Introduction

In this lab, we further developed our understanding of software testing by developing white box test cases to evaluate different code coverages. Our goal was to utilize the content covered in lecture to gain practical experience with white box testing.

# 2 Manual data-flow coverage calculations for X and Y methods

## Data-flow coverage calculations
#### Range.contains():
#### DataUtilities.calculateColumnTotal(): 

= DU-pairs tested / total DU-pairs 
= 17 / 28 = 60.71% DU pair coverage

### Work for 3.2 of the Lab Instructions
#### 3.2.1 and 3.2.2 Data Flow Graph and def-use pairs labelled at every node

#### DataUtilities.calculateColumnTotal():
![alt text](https://github.com/seng438-winter-2024/seng438-a3-dominico25/blob/main/DataUtilitiesDFG.jpg?raw=true)



#### 3.2.3 DU-pairs per variable

#### Range.contains()
| **Variable** | **DU-pairs**  |
|----------------|----------|
| this.lower | (0, 1), (0, 2) |
| this.upper | (0, 1), (0, 2) |
| value | (0, 1), (0, 2) |

#### DataUtilities.calculateColumnTotal()
| **Variable** | **DU-pairs**  |
|----------------|----------|
| data | (0, 1), (0, 2), (0, 7), (0, 12) |
| column | (0, 7), (0, 12) |
| total | (2, 9), (2, 14), (9, 9), (14, 14) |
| rowCount | (3, 5), (3, 11) |
| n | (7, 8), (7, 9), (12, 13), (12, 14) |
| r | (4, 5), (4, 7), (4, 10), (10, 10), (10, 8), (10, 9) |
| r2 | (6, 11), (6, 12), (6, 15), (15, 15), (15, 11), (15, 12) |

#### 3.2.4 Test cases and the pairs covered

#### Range.contains(double)
| **Test Case** | **DU-pairs covered**  |
|----------------|----------|
| testInRangeValueForMethodContains() | (0, 1), (0, 2) |
| testPositiveOutOfRangeValueForMethodContains() | (0, 1), (0, 2) |
| testNegativeOutOfRangeValueForMethodContains() | (0, 1), (0, 2) |
| testBelowUpperBoundaryForMethodContains() | (0, 1), (0, 2) |
| testEqualToUpperBoundaryForMethodContains() | (0, 1), (0, 2) |
| testAboveUpperBoundaryForMethodContains() | (0, 1), (0, 2) |
| testBelowLowerBoundaryForMethodContains() | (0, 1), (0, 2) |
| testEqualToLowerBoundaryForMethodContains() | (0, 1), (0, 2) |
| testAboveLowerBoundaryForMethodContains() | (0, 1), (0, 2) |
| testNominalValueForMethodContains() | (0, 1), (0, 2) |

#### DataUtilities.calculateColumnTotal()
| **Test Case** | **DU-pairs covered**  |
|----------------|----------|
| testCalculateColumnTotalForTwoValues() | (0, 1), (0, 2), (0, 7), (0, 7), (2, 9), (9, 9), (3, 5), (3, 11), (7, 8), (7, 9), (4, 5), (4, 7), (4, 10), (10, 10), (10, 8), (10, 9), (6, 11) |
| testCalculateColTotalForNegativeValues() | (0, 1), (0, 2), (0, 7), (0, 7), (2, 9), (9, 9), (3, 5), (3, 11), (7, 8), (7, 9), (4, 5), (4, 7), (4, 10), (10, 10), (10, 8), (10, 9), (6, 11) |
| testCalcColNullDataThrowsException() | (0, 1) |
| testCalcColumnNullObject() | (0, 1) |



# 3 A detailed description of the testing strategy for the new unit test


We used white-box testing to develop our new unit tests and improve the code coverage of our test suite. Our testing strategy started with analyzing the source code that we now have access to. We also ran our existing tests using the “run as coverage” functionality from the Eclipse EclEmma code coverage tool. The coverage metrics provided an understanding of the areas in the code that our existing tests were not thoroughly covering. Using the line, branch, and method coverage measurements and the goal percentages that needed to be met, we were able to identify the unit tests that needed to be added to our suite. Our team split up to work in detail with the different classes that needed to be tested. The White-box testing strategy changed how we went about writing unit tests. When going into detail with specific methods in the two classes, we were able to see exactly what lines were being missed, and the code that would need to execute in order to reach them. The source code gave us insight into the internal structure of the methods being tested. Using this information we were able to develop specific tests that cater to the different coverages we were measuring. An example of this strategy is identifying the decision points in a method that were missing coverage for a side of the branch. When locating this, we can then write a unit test that should enter that specific piece of logic and satisfy the condition, increasing the branch coverage of our test suite. This strategy was repeated and modified for each coverage type, creating a comprehensive set of unit tests that thoroughly cover the provided classes.


# 4 A high level description of five selected test cases you have designed using coverage information, and how they have increased code coverage

1. One test that was added to the DataUtilitiesTest file that increased the code coverage of the DataUtilities class was testGetCumulativePercentages_AllNullValues(). This test case targets the getCumulativePercentage(KeyedValues data) function, specifically parts of the function that deal with null values within the KeyedValues object that is taken in as an arguement. This was achieved by setting up a KeyedValues mock with 3 items of valid keys and values of null. Passing this object into the function increased branch coverage by hitting the "False" for most if/else statements within the function.

2. Another test that was added to the DataUtilitiesTest file that increased code coverage was the testEqual_SameContent() function. This test case targets the equal(double[][] a, double[][] b) function. This test case creates two new double[][] objects which contain the same content then passes those objects to the function. This tess is designed to hit the "False" on the first three if statements, then enetering the for loop but hitting the negative on the if statement within the loop. This function was not included in the Javadoc documentation that we initially used to write the test cases for this class in assignment 2, therefor this increased method coverage by 100%, branch coverage by ~50% and line coverage by ~60%. Eveything that this test case covered was an improvement to the original reported coverage.


# 5 A detailed report of the coverage achieved of each class and method (a screen shot from the code cover results in green and red color would suffice)

## Data Utilities
![image](https://github.com/seng438-winter-2024/seng438-a3-dominico25/assets/113636112/72746a58-2457-41b4-9559-270a38474a6f)

As seen in the images provided the overal class coverage percentages for our chosen metrics are:
    <br>Line coverage = 89.6% = ~90%
  <br>Branch coverage = 81.2%
  <br>Method Coverage = 100%

![image](https://github.com/seng438-winter-2024/seng438-a3-dominico25/assets/113636112/dc62d40e-bd01-4878-bc11-0ef9a610608f)

### Methods with 100% coverage on all three chosen metrics (Line, Branch, Method):
  <br>clone(double[][])
  <br>createNumberArray(double[])
  <br>createNumberArray2D(double[][])
  <br>equal(double[][], double[][])
  <br>calculateRowTotal(Values2D, int, int[])
<br>
### Methods without 100% coverage on all three chosen metrics (Line, Branch, Method):
#### calculateColumnTotal(Values2D, int)
![image](https://github.com/seng438-winter-2024/seng438-a3-dominico25/assets/113636112/85587e9d-c22f-4b4c-964d-1fea451bc804)

#### calculateRowTotal(Values2D, int)
![image](https://github.com/seng438-winter-2024/seng438-a3-dominico25/assets/113636112/148bb45d-4d26-45c4-9269-64b41157d2a7)

#### getCumulatievPercentage(KeyedValues)
![image](https://github.com/seng438-winter-2024/seng438-a3-dominico25/assets/113636112/c86f5283-7b5f-438f-ae50-73e67a9fa1d4)

#### calculateColumnTotal(Values2D, int, int[])
![image](https://github.com/seng438-winter-2024/seng438-a3-dominico25/assets/113636112/80da0283-0ba7-407a-907b-2dbcc0ba3f07)







## Range


# 6 Pros and Cons of coverage tools used and Metrics you report

The coverage tool used to conduct our white-box testing is Eclipse's EclEmma. This tool was convenient as it was already installed with Eclipse and it was very intuitive to become familiar with. Another advantage to this coverage tool is that it showed exactly where in the code our tests were not covering. We could identify specific lines in methods that required further testing. A con of EclEmma is that it does not include reports for the condition coverage metric. We were considering recording condition coverage when improving our test suite, but we chose to use method coverage instead. This was partially due to EclEmma not supporting condition coverage, but also because of the positive features of method coverage. Method coverage focuses on testing the methods in code, and if they have been executed by unit tests. Method coverage was useful in our test development as it allowed us to target the methods that had not been properly tested by our existing tests. The other metrics we use are branch and line coverage. A pro of using branch coverage is that we could see how the code proceeds at every decision point within the tested methods. This ensures that our test suite extensively tests the logical decisions made in the classes being tested. Line coverage was useful in ensuring at a base level that our tests would get through the bulk of the source code. This meant that our tests would run through the largest amount of lines that we could possibly test in the given classes. A negative to line testing is that if certain lines in the code at hand cannot possibly be reached by properly written tests, the line coverage metric will suffer. 

# 7 A comparison on the advantages and disadvantages of requirements-based test generation and coverage-based test generation.

Text…

# 8 A discussion on how the team work/effort was divided and managed

Our team managed the work effort first by working all together to set up the testing environment on each individuals machines. Raina created the data flow graph for the method calculateColumnTotal because she was the most familiar with it. Domo created the other data flow graph for contains because he created the tests for it in the last assignment. Increasing code coverage of our test suite started with each person trying to increase the tests they created in the previous assignment. Once we had completed our tests we proceeded to start completing test cases for the Range Class methods that we had not done in the previous assignment. This got us to the necessary coverage milestones.

# 9 Any difficulties encountered, challenges overcome, and lessons learned from performing the lab

Text…

# 10 Comments/feedback on the lab itself

We found this lab to be well organized and have a good workflow structure, we felt the work expectations were clear and had a good understanding of what was expected during the in lab demo. The supplementary powerpoint offered a good overview for the learning goals of the assignment.  We gained a better understanding of white box testing and how to use the JUnit framework. Overall this was an enjoyable lab that was understandable and easy to work on in a group.
